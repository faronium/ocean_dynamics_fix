{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ab201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prep_write_AR6_fig9_26_netcdfs(infilename,veldomain=False):\n",
    "    \"\"\"\n",
    "    Brings in the AR6 component NetCDF files and corrects the data arrangement to match that of the \n",
    "    AR5 and AR6 data that we are working with. Importantly, the data were transposed and also have\n",
    "    a duplicate data column in the longitude dimension to force the wrap-around in matlab. \n",
    "\n",
    "    This function takes the name of the file that is to be processed and also a flag if the data are\n",
    "    to be placed on the domain of the velocity data or the slightly more contrained domain for \n",
    "    mapping. Output is a xarray dataset with attributes taken from the original and additional\n",
    "    attributes given the lat and lon coordinates.\n",
    "    \"\"\"\n",
    "    #Rebuild this bag of arse from the ground up:\n",
    "    # Create the xarray Dataset (code from MS copilot)\n",
    "    dsin = xr.open_dataset(infilename)\n",
    "    dsin = dsin.rename({'Longitude': 'lon', 'Latitude': 'lat',})\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"SL_change\": ((\"lat\", \"lon\"), np.transpose(dsin.SL_change[0:360,:].values)),\n",
    "        },\n",
    "        coords={\n",
    "            \"lon\": ((\"lon\",), dsin.lon[0:360].values),\n",
    "            \"lat\": ((\"lat\",), dsin.lat.values),\n",
    "        },\n",
    "    )\n",
    "    ds = ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "    ds = ds.rio.write_crs(\"epsg:4326\")\n",
    "    #Add correct attributes for the lat/lon coordinates\n",
    "    ds = add_latlon_attrs(ds)\n",
    "\n",
    "    #Interpolate it to commonly used tenth degree grid.\n",
    "    #ds = interp_canada_tenth_degree(ds,'nearest',veldomain)\n",
    "    \n",
    "    #Add the attributes from the parent data to the Data variable\n",
    "    ds.SL_change.attrs['title'] = dsin.attrs['title']\n",
    "    ds.SL_change.attrs['units'] = dsin.attrs['units']\n",
    "    ds.SL_change.attrs['creator'] = dsin.attrs['creator']\n",
    "    ds.SL_change.attrs['activity'] = dsin.attrs['activity']\n",
    "    ds.SL_change.attrs['comments'] = dsin.attrs['comments']\n",
    "    ds.SL_change.attrs['long_name'] = infilename.split('_')[-3:-1][1] + ' sea-level contribution'\n",
    "    #Change the name of the dataset to prevent clobber when merged with other data\n",
    "    #split the file on . and keep the second to last\n",
    "    dsetname = infilename.split('_')[-3:-1][0]+'_'+infilename.split('_')[-3:-1][1] + '_slr'\n",
    "    ds = ds.rename({'SL_change': dsetname})\n",
    "    basename = infilename.split('.')[0]\n",
    "    outfilename = basename + '_ll_fixed.nc'\n",
    "    ds.to_netcdf(outfilename)\n",
    "    return ds\n",
    "\n",
    "def add_latlon_attrs(ds):\n",
    "    \"\"\"\n",
    "    Simple code to add the correct attributes to an xarray spatial grid \n",
    "    with dimensions lat and lon.\n",
    "\n",
    "    May write an extension of this function to correctly detect if lat/lon \n",
    "    exist in the file and modify the existing names to the conforming name lat/lon.\n",
    "    The existince of a billion names for these ever-so-common grid dimensions\n",
    "    is hyper annoying.\n",
    "\n",
    "    Takes an xarray dataset as input and assigns coordinates.\n",
    "    \"\"\"\n",
    "    commonlats = ['Lat','lat','latitude','Latitude','lats', 'Lats']\n",
    "    commonlons = ['Lon','lon', 'Long', 'long', 'longitude','longitude','lons', 'Lons', 'longs', 'Longs']\n",
    "    \n",
    "    #Test to see if lat and lon exist in the names of the \n",
    "    if ('lat' in list(ds.coords)) & ('lon' in list(ds.coords)):\n",
    "        #We have the required latitude and longitude names\n",
    "        ds.lon.attrs['standard_name'] = 'longitude'\n",
    "        ds.lon.attrs['long_name'] = 'longitude'\n",
    "        ds.lon.attrs['units'] = 'degrees_east'\n",
    "        ds.lon.attrs['axis'] = 'X'\n",
    "        ds.lat.attrs['standard_name'] = 'latitude'\n",
    "        ds.lat.attrs['long_name'] = 'latitude'\n",
    "        ds.lat.attrs['units'] = 'degrees_north'\n",
    "        ds.lat.attrs['axis'] = 'Y'\n",
    "        return ds\n",
    "    else:\n",
    "        #For now, do nothing. Maybe change to alter the coord that is there to \n",
    "        #the desired name and then alter as above.\n",
    "        return ds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_prep_write_AR6_fig9_26_netcdfs('C:/Users/fanslow/Work/SLR/Chapter-9/Plotting_code_and_data/Fig9_26_SL_regional/Plotted_Data/Fig9-26_data_ssp585_oceandynamics_map.nc')\n",
    "\n",
    "\n",
    "def print_latnodedata(latnodes,quantile):\n",
    "    print(ds.sel(locations=ds.locations.isin(latnodes),quantiles=quantile,years=2100).lat.values[0],\n",
    "          ds.sel(locations=ds.locations.isin(latnodes),quantiles=quantile,years=2100).sea_level_change.values)\n",
    "#    print(ds.sel(locations=ds.locations.isin(latnodes),quantiles=0.50,years=2100).sea_level_change.values)\n",
    "quantile = 0.5    \n",
    "latnodes = [173]\n",
    "#latnodes = [1004102870,1004102880,1004102890,1004102900,1004102910,1004102920,1004102930]\n",
    "print('----',ds.sel(locations=ds.locations.isin(latnodes),quantiles=quantile,years=2100).lon.values)   \n",
    "print_latnodedata(latnodes,quantile)\n",
    "latnodes = [i + 100000 for i in latnodes]\n",
    "print_latnodedata(latnodes,quantile)\n",
    "latnodes = [i + 100000 for i in latnodes]\n",
    "print_latnodedata(latnodes,quantile)\n",
    "latnodes = [i + 100000 for i in latnodes]\n",
    "print_latnodedata(latnodes,quantile)\n",
    "latnodes = [i + 100000 for i in latnodes]\n",
    "print_latnodedata(latnodes,quantile)\n",
    "# print(ds.sel(locations=ds.locations.isin([1004202880,1004202890,1004202900,1004202910,1004202920]),quantiles=0.50,years=2100).lat.values)\n",
    "# print(ds.sel(locations=ds.locations.isin([1004202880,1004202890,1004202900,1004202910,1004202920]),quantiles=0.50,years=2100).lon.values)\n",
    "# print(ds.sel(locations=ds.locations.isin([1004202880,1004202890,1004202900,1004202910,1004202920]),quantiles=0.50,years=2100).sea_level_change.values)\n",
    "\n",
    "# print(ds.sel(locations=ds.locations.isin([1004302880,1004302890,1004302900,1004302910,1004302920]),quantiles=0.50,years=2100).lat.values)\n",
    "# print(ds.sel(locations=ds.locations.isin([1004302880,1004302890,1004302900,1004302910,1004302920]),quantiles=0.50,years=2100).lon.values)\n",
    "# print(ds.sel(locations=ds.locations.isin([1004302880,1004302890,1004302900,1004302910,1004302920]),quantiles=0.50,years=2100).sea_level_change.values)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
